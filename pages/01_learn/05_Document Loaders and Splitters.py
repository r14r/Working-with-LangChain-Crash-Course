import os
import tempfile
import streamlit as st
from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter, Language
from langchain_community.document_loaders import TextLoader, CSVLoader, PyPDFLoader, WebBaseLoader
from lib.helper_streamlit.show_source import show_source

st.set_page_config(
    page_title="Document Loader & Text Splitter",
    page_icon="ğŸ“„"
)

st.header('ğŸ“„ Dokumente Laden & Intelligent Splitten')

# Create tabs
tab1, tab2 = st.tabs(["ğŸ“± App", "ğŸ“„ Source Code"])

with tab1:

    st.write('''
    **Das Problem:** LLMs haben begrenzte Kontextfenster und kÃ¶nnen nicht beliebig lange
    Dokumente verarbeiten. **Die LÃ¶sung:** Dokumente intelligent laden und in semantisch
    sinnvolle Chunks aufteilen.

    **In diesem Modul lernst du:**
    - ğŸ“‚ Verschiedene Dokumenttypen laden (PDF, CSV, TXT, Web)
    - âœ‚ï¸ Text-Splitting-Strategien
    - ğŸ¯ Chunk-Size und Overlap optimieren
    - ğŸ” Dokumente fÃ¼r RAG vorbereiten
    ''')

    st.info("ğŸ’¡ Gutes Splitting ist entscheidend fÃ¼r die QualitÃ¤t von RAG-Systemen!", icon="ğŸ’¡")

    st.divider()

    # -------------------------------------------------------------------
    # Document Loaders
    # -------------------------------------------------------------------
    st.subheader('ğŸ“¥ Document Loaders')

    st.write('''
    LangChain bietet **Document Loader** fÃ¼r die gÃ¤ngigsten Formate. Alle Loader
    konvertieren Inhalte in ein einheitliches `Document`-Objekt mit:
    - `page_content`: Der eigentliche Text
    - `metadata`: ZusÃ¤tzliche Infos (Quelle, Seite, etc.)
    ''')

    st.code('''
    from langchain_community.document_loaders import PyPDFLoader

    loader = PyPDFLoader("dokument.pdf")
    documents = loader.load()

    # Jedes Document hat:
    print(documents[0].page_content)  # Text
    print(documents[0].metadata)       # {'source': '...', 'page': 1}
    ''', language='python')

    st.markdown("### ğŸ“ TextLoader")

    txt_file = st.file_uploader("ğŸ“„ TXT-Datei hochladen", type=["txt"], key="txt")

    if txt_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".txt") as tmp:
            tmp.write(txt_file.read())
            tmp_path = tmp.name
    
        loader = TextLoader(tmp_path)
        docs = loader.load()
    
        st.success(f"âœ… {len(docs)} Dokument(e) geladen")
    
        with st.expander("ğŸ“„ Dokument ansehen"):
            for i, doc in enumerate(docs):
                st.markdown(f"**Dokument {i+1}**")
                st.text(doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content)
                st.json(doc.metadata)
    
        os.remove(tmp_path)

    st.markdown("### ğŸ“Š CSVLoader")

    csv_file = st.file_uploader("ğŸ“Š CSV-Datei hochladen", type=["csv"], key="csv")

    if csv_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp:
            tmp.write(csv_file.read())
            tmp_path = tmp.name
    
        loader = CSVLoader(tmp_path)
        docs = loader.load()
    
        st.success(f"âœ… {len(docs)} Zeile(n) als Dokumente geladen")
    
        with st.expander("ğŸ“Š Erste 3 Zeilen"):
            for doc in docs[:3]:
                st.text(doc.page_content)
                st.caption(f"Metadata: {doc.metadata}")
    
        os.remove(tmp_path)

    st.markdown("### ğŸ“• PyPDFLoader")

    pdf_file = st.file_uploader("ğŸ“• PDF-Datei hochladen", type=["pdf"], key="pdf")

    if pdf_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(pdf_file.read())
            tmp_path = tmp.name
    
        with st.spinner("PDF wird verarbeitet..."):
            loader = PyPDFLoader(tmp_path)
            docs = loader.load()
    
        st.success(f"âœ… {len(docs)} Seite(n) geladen")
    
        page_select = st.selectbox("Seite auswÃ¤hlen", range(len(docs)))
    
        with st.expander(f"ğŸ“„ Seite {page_select + 1}"):
            st.text(docs[page_select].page_content)
            st.json(docs[page_select].metadata)
    
        os.remove(tmp_path)

    st.markdown("### ğŸŒ WebBaseLoader")

    with st.form("web_loader"):
        url_input = st.text_input(
            "URL eingeben",
            placeholder="https://example.com/artikel"
        )
        load_btn = st.form_submit_button("ğŸŒ Webseite laden")
    
        if load_btn and url_input:
            with st.spinner("Lade Webseite..."):
                try:
                    loader = WebBaseLoader(url_input)
                    docs = loader.load()
                
                    st.success(f"âœ… Webseite geladen ({len(docs[0].page_content)} Zeichen)")
                
                    with st.expander("ğŸ“„ Inhalt ansehen"):
                        st.text(docs[0].page_content[:1000] + "...")
                except Exception as e:
                    st.error(f"âŒ Fehler beim Laden: {e}")

    st.divider()

    # -------------------------------------------------------------------
    # Text Splitters
    # -------------------------------------------------------------------
    st.subheader('âœ‚ï¸ Text Splitters')

    st.write('''
    **Warum splitten?**
    - ğŸ“ LLMs haben begrenzte Kontextfenster (z.B. 4k, 8k, 32k Tokens)
    - ğŸ¯ Kleinere Chunks â†’ prÃ¤zisere Retrieval-Ergebnisse
    - ğŸ’° Weniger Tokens â†’ geringere Kosten (bei API-basierten LLMs)
    - ğŸ§  Bessere semantische GranularitÃ¤t

    **Strategien:**
    - **CharacterTextSplitter**: Einfaches Splitting an Zeichen
    - **RecursiveCharacterTextSplitter**: Intelligent (AbsÃ¤tze â†’ SÃ¤tze â†’ WÃ¶rter)
    - **Language-aware**: Code-spezifisches Splitting
    ''')

    st.markdown("### ğŸ”ª CharacterTextSplitter")

    st.write("Splittet Text an einem festen Trennzeichen (z.B. Leerzeichen, Newline).")

    with st.form("char_splitter"):
        text1 = st.text_area(
            "Text eingeben",
            height=150,
            placeholder="FÃ¼ge einen lÃ¤ngeren Text ein zum Splitten..."
        )
    
        col1, col2 = st.columns(2)
        with col1:
            chunk_size1 = st.slider("Chunk-GrÃ¶ÃŸe", 20, 200, 80, key="cs1")
        with col2:
            overlap1 = st.slider("Overlap", 0, 50, 10, key="o1")
    
        split_btn1 = st.form_submit_button("âœ‚ï¸ Splitten")
    
        if split_btn1 and text1.strip():
            splitter = CharacterTextSplitter(
                separator=" ",
                chunk_size=chunk_size1,
                chunk_overlap=overlap1,
                length_function=len
            )
        
            chunks = splitter.split_text(text1)
        
            st.success(f"âœ… {len(chunks)} Chunks erstellt")
        
            for i, chunk in enumerate(chunks):
                with st.expander(f"Chunk {i+1} ({len(chunk)} Zeichen)"):
                    st.text(chunk)

    st.markdown("### ğŸ§  RecursiveCharacterTextSplitter (Empfohlen)")

    st.write('''
    Der **intelligente Splitter**: Versucht zuerst an AbsÃ¤tzen zu trennen, dann an
    SÃ¤tzen, dann an WÃ¶rtern. ErhÃ¤lt semantischen Zusammenhang besser.
    ''')

    st.code('''
    from langchain_text_splitters import RecursiveCharacterTextSplitter

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,        # Max. Zeichen pro Chunk
        chunk_overlap=200,      # Ãœberlappung fÃ¼r Kontext
        separators=["\\n\\n", "\\n", ". ", " ", ""]  # Hierarchie
    )

    docs = splitter.create_documents([text])
    ''', language='python')

    with st.form("recursive_splitter"):
        text2 = st.text_area(
            "Text mit AbsÃ¤tzen eingeben",
            height=200,
            placeholder="Absatz 1.\n\nAbsatz 2.\n\nAbsatz 3...",
            value="KÃ¼nstliche Intelligenz revolutioniert viele Branchen.\n\nMachine Learning ist ein Teilbereich der KI.\n\nDeep Learning nutzt neuronale Netze mit vielen Schichten."
        )
    
        col1, col2 = st.columns(2)
        with col1:
            chunk_size2 = st.slider("Chunk-GrÃ¶ÃŸe", 50, 500, 100, key="cs2")
        with col2:
            overlap2 = st.slider("Overlap", 0, 100, 20, key="o2")
    
        split_btn2 = st.form_submit_button("âœ‚ï¸ Intelligent splitten")
    
        if split_btn2 and text2.strip():
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size2,
                chunk_overlap=overlap2,
                separators=["\n\n", "\n", ". ", " ", ""]
            )
        
            docs = splitter.create_documents([text2])
        
            st.success(f"âœ… {len(docs)} Dokumente erstellt")
        
            for i, doc in enumerate(docs):
                with st.expander(f"ğŸ“„ Dokument {i+1}"):
                    st.text(doc.page_content)
                    st.caption(f"LÃ¤nge: {len(doc.page_content)} Zeichen")

    st.markdown("### ğŸ’» Code-Splitting")

    st.write("FÃ¼r Code gibt es spezialisierte Splitter, die Funktionen/Klassen respektieren.")

    st.code('''
    from langchain_text_splitters import RecursiveCharacterTextSplitter, Language

    python_splitter = RecursiveCharacterTextSplitter.from_language(
        language=Language.PYTHON,
        chunk_size=500,
        chunk_overlap=50
    )

    code_docs = python_splitter.create_documents([python_code])
    ''', language='python')

    st.divider()

    with st.expander("ğŸ¯ Best Practices fÃ¼r Splitting"):
        st.markdown("""
        **Chunk-Size wÃ¤hlen:**
        - ğŸ“š **Lange Dokumente** (BÃ¼cher): 1000-2000 Zeichen
        - ğŸ“„ **Artikel/Berichte**: 500-1000 Zeichen
        - ğŸ’¬ **Chat/FAQ**: 200-500 Zeichen
        - ğŸ’» **Code**: Funktions-/Klassen-basiert
    
        **Overlap einstellen:**
        - âœ… 10-20% der Chunk-Size ist ein guter Start
        - âœ… Mehr Overlap = mehr Kontext, aber auch Redundanz
        - âœ… Zu wenig = Informationsverlust an Grenzen
    
        **Embeddings berÃ¼cksichtigen:**
        - âš ï¸ Embedding-Modelle haben Max-Token-Limits
        - âš ï¸ `nomic-embed-text`: ~8k Tokens
        - âš ï¸ Chunk-Size sollte deutlich darunter liegen
        """)

    st.caption("Workshop-Material: Document Loading & Intelligent Text Splitting")

with tab2:
    st.markdown("### Source Code")
    show_source(__file__)
